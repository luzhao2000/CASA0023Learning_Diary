<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CASA0023 Learning Diary - 3&nbsp; Week 3 Corrections</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Week4.html" rel="next">
<link href="./Week2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week 3 Corrections</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CASA0023 Learning Diary</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Content</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Introduction</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Week 1 Getting started with remote sensing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Week 2 Portfolio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week3.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week 3 Corrections</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week 4 Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week 5 Google Earth Engine I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week6 Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week7.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week 7 Classification II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Week 8 Temperature and policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary"><span class="toc-section-number">3.1</span>  Summary</a>
  <ul class="collapse">
  <li><a href="#summary-for-lecture" id="toc-summary-for-lecture" class="nav-link" data-scroll-target="#summary-for-lecture"><span class="toc-section-number">3.1.1</span>  Summary for lecture</a></li>
  <li><a href="#summary-for-practical" id="toc-summary-for-practical" class="nav-link" data-scroll-target="#summary-for-practical"><span class="toc-section-number">3.1.2</span>  Summary for practical</a></li>
  <li><a href="#questions" id="toc-questions" class="nav-link" data-scroll-target="#questions"><span class="toc-section-number">3.1.3</span>  Questions</a></li>
  </ul></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="toc-section-number">3.2</span>  Application</a>
  <ul class="collapse">
  <li><a href="#application-of-atmosphere-correction" id="toc-application-of-atmosphere-correction" class="nav-link" data-scroll-target="#application-of-atmosphere-correction"><span class="toc-section-number">3.2.1</span>  Application of atmosphere correction</a></li>
  <li><a href="#application-of-image-enhancement" id="toc-application-of-image-enhancement" class="nav-link" data-scroll-target="#application-of-image-enhancement"><span class="toc-section-number">3.2.2</span>  Application of image enhancement</a></li>
  </ul></li>
  <li><a href="#reflection" id="toc-reflection" class="nav-link" data-scroll-target="#reflection"><span class="toc-section-number">3.3</span>  Reflection</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week 3 Corrections</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this week, we learnt how to pre-process remote sensing imagery based on four types of corrections, feathering, and image enhancement. This part is very important because if the pre-processing is not done well, it may affect the subsequent analysis and the accuracy of the final results.</p>
<section id="summary" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.1</span> Summary</h2>
<p>Summary part of the learning diary in this week including two sections, one is summary for lecture, the other is summary for practical.</p>
<section id="summary-for-lecture" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="summary-for-lecture"><span class="header-section-number">3.1.1</span> Summary for lecture</h3>
<p>Different sensors may have different ways of taking images, some are similar to push broom (Fig.1), such as SPOT and Quickbird, while sensors like Landsat satellites collect data like whisk broom (Fig.1), in which case the scan line corrector (SLC) is important to control and correct the course and direction of the scan line. Since Landsat 7 did not add SLC, there are gaps between the collected data (Fig.2) and it is difficult to fill in the missing data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_sensor.png" class="img-fluid figure-img" width="300"></p>
<p></p><figcaption class="figure-caption">Fig.1 principle of push broom and whisk broom sensor. Source: <span class="citation" data-cites="indiawaterportal2012">India Water Portal (<a href="references.html#ref-indiawaterportal2012" role="doc-biblioref">2012</a>)</span></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_slc.png" class="img-fluid figure-img" width="300"></p>
<p></p><figcaption class="figure-caption">Fig.2 Scan line with and without the scan line corrector. Source: <span class="citation" data-cites="landsatmissions">Landsat Missions (<a href="references.html#ref-landsatmissions" role="doc-biblioref">n.d.</a>)</span></figcaption><p></p>
</figure>
</div>
<section id="part1-imagery-corrections" class="level4" data-number="3.1.1.1">
<h4 data-number="3.1.1.1" class="anchored" data-anchor-id="part1-imagery-corrections"><span class="header-section-number">3.1.1.1</span> Part1: imagery corrections</h4>
<p>Depending on the quality of the acquired data and the needs of the user, the collected raw data is usually first corrected, which includes geometric correction, atmospheric correction, topological correction and radiometric calibration.</p>
<ul>
<li>Geometric correction</li>
</ul>
<p>We need to do geometric correction is because the following four conditions may occur during the data collection process to cause image distortions. First is the capture of images due to the difference in view angles may cause the shadow of the objects on the image or the shadow of the aircraft sensors. Secondly, it is influenced by the topography, for example, the mountain is not a flat surface but has to be “printed” on a two-dimensional image. Thirdly, affected by weather, such as winds, heavy rain and other bad weather, sensors such as those carried by airplanes and balloons are unable to collect data stably, which can also cause image distortion. Last but not least, it is also possible that the rotation of the Earth makes the adjacent images misaligned and not perfectly aligned.</p>
<p>The principle of geometric correction is to determine some ground control points (GCP) with coordinates after selecting the target image (uncorrected image) and the reference image (corrected image), which means that we need to collect GPS data as well. In addition, to establish a function (e.g linear, polynomial, projective, etc.) based on the mathematical relationship between the positions of these GCPs in the uncorrected image and corrected image. The complete image is corrected according to this function, and the geometrically corrected image is obtained. Following Figure 3 is an example for comparing the difference between the same image before and after geometric correction.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_geometric_cor.png" class="img-fluid figure-img" width="400"></p>
<p></p><figcaption class="figure-caption">Fig.3 Comparison between the same image before and after geometric correction. Source: <span class="citation" data-cites="xue2021">Xue, Wang, and Zhong (<a href="references.html#ref-xue2021" role="doc-biblioref">2021</a>)</span></figcaption><p></p>
</figure>
</div>
<p>Notice:</p>
<ol type="1">
<li>It is also needed to minimize the RMSE value, usually less than 0.5. Add more GCPs might decrease the RMSE.</li>
<li>The points after rectified may not fall on the grid squares, and even some might fall outside the boundary. To solve this problem, we can do re-sample based on suitable methods to let the pixels in the output grids have normal values.</li>
</ol>
<ul>
<li>Atmospheric correction</li>
</ul>
<p>According to <span class="citation" data-cites="jensen2015">Jensen (<a href="references.html#ref-jensen2015" role="doc-biblioref">2015</a>)</span>, atmospheric scattering and topographic attenuation are the most two important sources of environment attenuation. We perform atmospheric correction to reduce the effect of clouds on the image and reduce the contrast of the image. For example, since clouds are very bright, they would cause an adjacency effect that makes the surrounding pixels brighter than they really are. There are two types of atmospheric correction, one is relative atmospheric correction, the other is absolute atmospheric correction. Absolute atmospheric correction (e.g.&nbsp;FLAASH, empirical line correction) is changing digital brightness values into scaled surface reflectance, which can use atmospheric radiative transfer models, but it would be really hard, since it needs a lot of data and a long-time period to do this.</p>
<p>Dark object subtraction (DOS) and Psuedo-invariant Features (PIFs) are two relative atmospheric correction methods. Dark object subtraction (DOS) is based on the assumption that the reflectance of dark objects in images (e.g., bodies of water, shadows of mountains, etc.) has almost no reflectance in some specific wavelength bands, and therefore the reflectance corresponding to these dark objects in the image should be a fraction of the energy absorbed by the sensor after the solar radiation is directly reflected by the atmosphere <span class="citation" data-cites="blamannen2011">(<a href="references.html#ref-blamannen2011" role="doc-biblioref">blamannen 2011</a>)</span>. Therefore, by subtracting the value corresponding to the darkest object pixel in the image, a higher contrast image that is closer to the real situation is obtained. While PIFs are matching the brightness pixels in the remote sensing images and the spectral reflectance data measured simultaneously using a field spectrometer in the linear relationship, and use linear regression to adjust (predict) the values of other pixels in the image to be corrected.</p>
<p>However, it is not always necessary to do atmospheric correction, such as when classifying a single image, independent classification of multiple date imagery, and other situation <span class="citation" data-cites="jensen2015">(<a href="references.html#ref-jensen2015" role="doc-biblioref">Jensen 2015</a>)</span>.</p>
<ul>
<li>Topological (orthorectification) correction</li>
</ul>
<p>The essence of topological correction is to computationally adjust the image looks same as the view at nadir look and remove distortions, which requires sensor geometry and an elevation model. Topological correction can be done by using software like QGIS and SAGA GIS, or R as well.</p>
<p>Notice: Atmospheric correction should be done before topological correction.</p>
<ul>
<li>Radiometric calibration</li>
</ul>
<p>Radiometric calibration is based on the raw data collected from the satellites to change digital numbers (DN) into spectral radiance, which means that change image brightness into the amount of light within a band from a sensor in the field of view (FOV). After the transmission, the spectral radiance is with units and meaning.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notice
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>DNs do not have units.</li>
<li>Radiance is usually called top of atmosphere radiance.</li>
<li>Level 2 product refers to the data has been changed or advanced.</li>
</ol>
</div>
</div>
</section>
<section id="part2-data-joining-and-enhancement" class="level4" data-number="3.1.1.2">
<h4 data-number="3.1.1.2" class="anchored" data-anchor-id="part2-data-joining-and-enhancement"><span class="header-section-number">3.1.1.2</span> Part2: data joining and enhancement</h4>
<p>Data joining is like merging imagery together. In many cases, we need to join data sets to generate a complete remote sensing image covering the whole study area when the study area is relatively large or located at the junction of multiple images. In the process of joining, the average of the pixel values of the overlapping parts is usually chosen as the pixel value of the joined images. The reason for doing so is to reduce the difference of DN values between the two images.</p>
<p>Image enhancement refers to the enhancement of image contrast because many different materials may have little difference in their ability to reflect the same wavelength of energy, which may cause the range of DNs of the raw image to be concentrated in a smaller range, resulting in a lack of significant contrast and the need for contrast enhancement. Common methods include expand the range of DNs from Minimum to Maximum, percentage linear and standard deviation, and piecewise linear contrast stretch. In addition, there are many other enhancement which can be applied to improve the quality and visual appearance of the imagery, including ratio enhancement, filtering (e.g.&nbsp;edge enhancement), PCA texture enhancement, pan sharpen and data fusion (decision level fusion, object-level fusion, image fusion and satellites data fusion).</p>
<p>Notice: Image stretch only change how the image express by stretching the range of original image in a more suitable range, it does not change the raw image.</p>
</section>
</section>
<section id="summary-for-practical" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="summary-for-practical"><span class="header-section-number">3.1.2</span> Summary for practical</h3>
<p>In this week’s practical part, we mainly focus on three parts, including atmosphere correction, imagery merge, and image enhancement. In practical applications, these are important steps for the pre-processing of acquired original remote sensing images, because it is never possible to obtain the most ideal image. The main workflow of practical in this week shows in the following Figure 4. According to the workflow, it shows that our main task is to actually apply these image processing methods, review what we talked about in the lecture and what they should be used for.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_practical_workflow.jpg" class="img-fluid figure-img" width="600"></p>
<p></p><figcaption class="figure-caption">Fig.4 Workflow of Practical in Week 3</figcaption><p></p>
</figure>
</div>
<p>According to Figure 4 above, I divided this practical into 3 parts. The first part is for atmosphere correction, and I focused on one of the atmosphere correction methods called Dark Object Subtraction (DOS) in this week. As mentioned above in the summary of lecture part, DOS is an approach based on the fact that very dark objects do not reflect any energy back to the sensor, so subtract that energy and the image appears unaffected by the atmosphere. Following is the R code that we can use to do DOS in R, and based on the function radCor( ) and relevant parameter settings, we can get the result after DOS in a quick way without manual calculation. In addition to DOS, it is also mentioned to convert the DN value of the image directly into reflectance, that is, the reflectance of the Top of Atmosphere.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#boa_ref &lt;- radCor(lsatMeta, metaData, method = "dos")# surface reflectance with DOS</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#lsat_rad &lt;- radCor(lsatMeta, metaData = metaData, method = "rad")# Radiance</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#hazeDN    &lt;- RStoolbox::estimateHaze(lsat, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#lsat_sref &lt;- radCor(lsatMeta, metaData = metaData, method = "dos",hazeValues = hazeDN, hazeBands = 2:4)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In part 2, after I did pre-processing for the imagery that I collected, I merged two images by using mosaic( ) function in the terra package in R. In this case, the method of mean value is adopted to deal with the overlapping parts between two images. In part 3, I tried 5 image enhancement approaches, including ratio enhancement, filtering, texture enhancement, data fusion &amp; PCA. In the part of ratio enhancement, I used Normalised Difference Vegetation Index (NDVI) to enhance the vegetation coverage in the image (Figure 5), while Normalized Difference Moisture Index (NDMI) could enhance the area with high moisture in the image (Figure6). in this case, the results of NDVI and NDMI looks similar.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_NDVI_2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.5 Area with NDVI value higher than 0.2 (relative high vegetation cover rate in Cape Town)</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_NDMI_2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.6 Area with NDMI value lower than -0.2 (relatively high mositure in Cape Town)</figcaption><p></p>
</figure>
</div>
<p>For texture enhancement, I chose Band 4. According to the spectral feature curve of the surface object, the enhanced part should be green vegetation, which can be confirmed by Figure 7 below. This method may be used for image enhancement with higher resolution and large differences in surface objects, which may get more obvious enhancement effect.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_texture_enhancement.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.7 Texture enhancement result</figcaption><p></p>
</figure>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notice
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Formula of DOS, and the meanings of these indices in the formula.</li>
<li>Usually, do not use panchromatic band when doing land cover classification.</li>
</ol>
</div>
</div>
</section>
<section id="questions" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="questions"><span class="header-section-number">3.1.3</span> Questions</h3>
<ol type="1">
<li>How many GCPs do we need to select when doing geometric correction?</li>
<li>Why atmospheric correction is not always necessaryly done?</li>
<li>What is the difference between empirical line correction and PIFs, both of them correct image based on regression model?</li>
<li>How to convert TOA radiance to TOA reflectance?</li>
<li>What is the light source when mention TOA reflectance and surface reflectance?</li>
<li>Is pan sharpen a kind of high pass enhancement? if not, what is the difference between them?</li>
<li>Why color of pixels outside the overlapping area of the following images also changed after joining?</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_join_data.png" class="img-fluid figure-img" width="600"></p>
<p></p><figcaption class="figure-caption">Fig.8 Comparison between two images before and after joining.</figcaption><p></p>
</figure>
</div>
<ol start="8" type="1">
<li>After doing PCA, how to interpret the results? What is the components refer to?</li>
</ol>
</section>
</section>
<section id="application" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="application"><span class="header-section-number">3.2</span> Application</h2>
<p>In this week’s Application section, I mainly supplemented several cases of atmospheric correction and image enhancement in practical research, so as to help me better understand relevant theoretical knowledge and know more about atmosphere correction and image enhancement.</p>
<section id="application-of-atmosphere-correction" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="application-of-atmosphere-correction"><span class="header-section-number">3.2.1</span> Application of atmosphere correction</h3>
<p><span class="citation" data-cites="pereira-sandoval2019">Pereira-Sandoval et al. (<a href="references.html#ref-pereira-sandoval2019" role="doc-biblioref">2019</a>)</span> decided to find a more appropritiate method for atmospheric correction with inland water bodies based on Sentinel-2 multispectral images and obtain a higher accuracy. Different from the ocean, the surface-leave radiance of inland water and water near the coast to the near infrared band is usually greater than 0, so the DOS method is not much suitable to be used for atmospheric correction of the image <span class="citation" data-cites="pereira-sandoval2019">(<a href="references.html#ref-pereira-sandoval2019" role="doc-biblioref">Pereira-Sandoval et al. 2019</a>)</span>. After comparing six atmospheric correction processors, <span class="citation" data-cites="pereira-sandoval2019">Pereira-Sandoval et al. (<a href="references.html#ref-pereira-sandoval2019" role="doc-biblioref">2019</a>)</span> concluded that in this Case the Polynomial-based algorithm applied to MERIS (Polymer) and Case 2 Regional Coast Color (here called C2RCC) was the best two performers. Although the data they chose is free and open source like Sentinel-2, the resolution of 10 to 20 meters is acceptable for the study of surface objects such as inland water bodies. However, the general applicability of these two methods is not high. Because the research area they choose is mainly for 8 reservoirs and 1 coastal lagoon (Figure 9), the accuracy of results cannot be guaranteed after atmospheric correction, since they may be affected by the water quality and water form of inland water bodies, and the accuracy of these two methods might be changed significantly. The problem might be solved by first classifying the images and then using different atmospheric correction methods <span class="citation" data-cites="pereira-sandoval2019">(<a href="references.html#ref-pereira-sandoval2019" role="doc-biblioref">Pereira-Sandoval et al. 2019</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_application1.png" class="img-fluid figure-img" width="600"></p>
<p></p><figcaption class="figure-caption">Fig.9 Eight reservoirs selected as part of the study area with different shape, color, and location Source: <span class="citation" data-cites="pereira-sandoval2019">Pereira-Sandoval et al. (<a href="references.html#ref-pereira-sandoval2019" role="doc-biblioref">2019</a>)</span></figcaption><p></p>
</figure>
</div>
<p>Dark-object subtraction (DOS) is a widely used and constantly improving atmospheric correction method <span class="citation" data-cites="zhang2010">(<a href="references.html#ref-zhang2010" role="doc-biblioref">Zhang, He, and Wang 2010</a>)</span>. <span class="citation" data-cites="ding2015">Ding et al. (<a href="references.html#ref-ding2015" role="doc-biblioref">2015</a>)</span> proposed a improved DOS method for atmospheric correction, which took into account Rayleigh scattering, Mie scattering and other remote sensing image parameters. The difference between this approach and the basic DOS method is that haze value used in this approach is a final predict value based on the mean DN value in each band and a predicted haze value which considered normalized gain and offset value <span class="citation" data-cites="ding2015">(<a href="references.html#ref-ding2015" role="doc-biblioref">Ding et al. 2015</a>)</span>. The difference between the improved DOS atmospheric correction method and other DOS can be seen directly from Figure 10 and 11 below. <span class="citation" data-cites="ding2015">Ding et al. (<a href="references.html#ref-ding2015" role="doc-biblioref">2015</a>)</span> also combined NDVI and NDWI to try to extract vegetation and water in the image after atmospheric correction, and they found that the improved DOS method was better. However, the selection of the study area is very simple, so it is impossible to judge whether such results can be obtained when this method is used for atmospheric correction of remote sensing images through this study and related results. In addition, it can be seen from the improved DOS image after atmospheric correction that the overall image is reddish, which may affect the subsequent quantitative remote sensing results and analysis, so further optimization may be needed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_application2_1.png" class="img-fluid figure-img" width="300"></p>
<p></p><figcaption class="figure-caption">Fig.10 Original satellite image. Source: <span class="citation" data-cites="ding2015">Ding et al. (<a href="references.html#ref-ding2015" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_application2_2.png" class="img-fluid figure-img" width="300"></p>
<p></p><figcaption class="figure-caption">Fig.11 Image after implemented improved DOS atmosphere correction. Source: <span class="citation" data-cites="ding2015">Ding et al. (<a href="references.html#ref-ding2015" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</section>
<section id="application-of-image-enhancement" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="application-of-image-enhancement"><span class="header-section-number">3.2.2</span> Application of image enhancement</h3>
<p>Texture is a very important feature in the process of image interpretation and classification, and texture analysis has been widely used in vegetation related research <span class="citation" data-cites="feng2015 pradhan2014">(<a href="references.html#ref-feng2015" role="doc-biblioref">Feng, Liu, and Gong 2015</a>; <a href="references.html#ref-pradhan2014" role="doc-biblioref">Pradhan et al. 2014</a>)</span>. When <span class="citation" data-cites="lu2012">Lu et al. (<a href="references.html#ref-lu2012" role="doc-biblioref">2012</a>)</span> classified land use and land cover in the Amazon jungle area of Brazil, they found that due to the mixed distribution of local land use and land cover, the result of direct classification was not ideal. While, textural image enhancement can significantly highlight different land use or land cover types, so they combine textural image and spectral signatures to improve classification accuracy <span class="citation" data-cites="lu2012">(<a href="references.html#ref-lu2012" role="doc-biblioref">Lu et al. 2012</a>)</span>. In the following Figure 12, A is tuextural image enhancement result based on near infrared band, and B is tuextural image enhancement result based on short wave infrared. The area covered by vegetation can be clearly seen from the figure, and the depth of color can also reflect the density of vegetation cover. This method can indeed help improve the accuracy of image classification. However, in the aspect of data selection, if we want to further improve the accuracy and accuracy of classification, we need to use higher resolution data, especially for the complex environment like the jungle, or the research area is small, or the research object is small.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week3_application3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Fig.12 Textural image enhancement result based on near infrared band (A) and shortwave infrared (B). Source: <span class="citation" data-cites="lu2012">Lu et al. (<a href="references.html#ref-lu2012" role="doc-biblioref">2012</a>)</span></figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="reflection" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="reflection"><span class="header-section-number">3.3</span> Reflection</h2>
<p>What is interesting?</p>
<p>I think the interesting thing in this week is that the image that the sensor gets can be significantly different from the actual scene, for example, due to the influence of particles in the atmosphere. However, the effect of atmosphere on image quality can be minimized by atmospheric correction. It can also improve the quality of the image through image enhancement, so that users can distinguish the ground objects more clearly. For example, by calculating NDVI, green vegetation with different chlorophyll contents can be roughly mastered, which can be used to monitor crop diseases and insect pests and other practical problems.</p>
<p>What is useful in the future?</p>
<p>Both atmospheric correction and image enhancement should be done after obtaining the original remote sensing image, but there may be differences according to different research purposes. For example, the color contrast of the image after atmospheric correction is more stark than that of the original image. Whether it is supervised classification, unsupervised classification or visual discrimination, it can be clearer and help to get more accurate classification results. And image enhancement also has a wide range of applications, such as the NDMI image can be preliminarily found flood areas.</p>
<p>What is not useful? (can be instead)</p>
<p>In my opinion, neither atmospheric correction nor image enhancement can be instead so far, but the quality of some original satellite data is very high or the research questions studied by some scholars are relatively special, so it may be unnecessary to carry out the image preprocessing of atmospheric correction and image enhancement.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-blamannen2011" class="csl-entry" role="doc-biblioentry">
blamannen. 2011. <span>“Landsat Data Processing (in ENVI).”</span> <a href="https://blamannen.wordpress.com/2011/05/01/landsat-data-processing-in-envi/">https://blamannen.wordpress.com/2011/05/01/landsat-data-processing-in-envi/</a>.
</div>
<div id="ref-ding2015" class="csl-entry" role="doc-biblioentry">
Ding, Huimei, Junsheng Shi, Yuanfangzhou Wang, and Li Wei. 2015. <span>“Ninth International Symposium on Multispectral Image Processing and Pattern Recognition (MIPPR2015).”</span> In, edited by Jianguo Liu and Hong Sun, 98150K. Enshi, China. <a href="https://doi.org/10.1117/12.2205567">https://doi.org/10.1117/12.2205567</a>.
</div>
<div id="ref-feng2015" class="csl-entry" role="doc-biblioentry">
Feng, Quanlong, Jiantao Liu, and Jianhua Gong. 2015. <span>“UAV Remote Sensing for Urban Vegetation Mapping Using Random Forest and Texture Analysis.”</span> <em>Remote Sensing</em> 7 (1): 1074–94. <a href="https://doi.org/10.3390/rs70101074">https://doi.org/10.3390/rs70101074</a>.
</div>
<div id="ref-indiawaterportal2012" class="csl-entry" role="doc-biblioentry">
India Water Portal. 2012. <span>“Remote Sensing Application in Agriculture &amp; Forestry<span>_</span>dr Menon a r r (<span>…</span>,”</span> December. <a href="https://www.slideshare.net/indiawaterportal/remote-sensing-application-in-agriculture-and-forestry-menon-a-r-r">https://www.slideshare.net/indiawaterportal/remote-sensing-application-in-agriculture-and-forestry-menon-a-r-r</a>.
</div>
<div id="ref-jensen2015" class="csl-entry" role="doc-biblioentry">
Jensen, John R. 2015. <em>Introductory Digital Image Processing: A Remote Sensing Perspective</em>. Old Tappan: Pearson Education.
</div>
<div id="ref-landsatmissions" class="csl-entry" role="doc-biblioentry">
Landsat Missions. n.d. <span>“Landsat 7.”</span> <a href="https://www.usgs.gov/landsat-missions/landsat-7">https://www.usgs.gov/landsat-missions/landsat-7</a>.
</div>
<div id="ref-lu2012" class="csl-entry" role="doc-biblioentry">
Lu, Dengsheng, Mateus Batistella, Guiying Li, Emilio Moran, Scott Hetrick, Corina da Costa Freitas, Luciano Vieira Dutra, and Sidnei João Siqueira Sant’Anna. 2012. <span>“Land Use/Cover Classification in the Brazilian Amazon Using Satellite Images.”</span> <em>Pesquisa Agropecuária Brasileira</em> 47 (9): 1185–1208. <a href="https://doi.org/10.1590/S0100-204X2012000900004">https://doi.org/10.1590/S0100-204X2012000900004</a>.
</div>
<div id="ref-pereira-sandoval2019" class="csl-entry" role="doc-biblioentry">
Pereira-Sandoval, Marcela, Ana Ruescas, Patricia Urrego, Antonio Ruiz-Verdú, Jesús Delegido, Carolina Tenjo, Xavier Soria-Perpinyà, Eduardo Vicente, Juan Soria, and José Moreno. 2019. <span>“Evaluation of Atmospheric Correction Algorithms over Spanish Inland Waters for Sentinel-2 Multi Spectral Imagery Data.”</span> <em>Remote Sensing</em> 11 (12): 1469. <a href="https://doi.org/10.3390/rs11121469">https://doi.org/10.3390/rs11121469</a>.
</div>
<div id="ref-pradhan2014" class="csl-entry" role="doc-biblioentry">
Pradhan, Biswajeet, Ulrike Hagemann, Mahyat Shafapour Tehrany, and Nikolas Prechtel. 2014. <span>“An Easy to Use ArcMap Based Texture Analysis Program for Extraction of Flooded Areas from TerraSAR-X Satellite Image.”</span> <em>Computers &amp; Geosciences</em> 63 (February): 34–43. <a href="https://doi.org/10.1016/j.cageo.2013.10.011">https://doi.org/10.1016/j.cageo.2013.10.011</a>.
</div>
<div id="ref-xue2021" class="csl-entry" role="doc-biblioentry">
Xue, Wu, Peng Wang, and lingyu Zhong. 2021. <span>“Geometric Correction of Optical Remote Sensing Satellite Images Captured by Linear Array Sensors Circular Scanning Perpendicular to the Orbit.”</span> <em>Changchun: Chinese Academy of Sciences</em> Vol.29 (12): p.2924–2934. <a href="https://d-wanfangdata-com-cn.libproxy.ucl.ac.uk/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjMwMTEyEg9neGptZ2MyMDIxMTIwMTcaCHBheDgxcGlx">https://d-wanfangdata-com-cn.libproxy.ucl.ac.uk/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjMwMTEyEg9neGptZ2MyMDIxMTIwMTcaCHBheDgxcGlx</a>.
</div>
<div id="ref-zhang2010" class="csl-entry" role="doc-biblioentry">
Zhang, Zhaoming, Guojin He, and Xiaoqin Wang. 2010. <span>“A Practical DOS Model-Based Atmospheric Correction Algorithm.”</span> <em>International Journal of Remote Sensing</em> 31 (11): 2837–52. <a href="https://doi.org/10.1080/01431160903124682">https://doi.org/10.1080/01431160903124682</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Week2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Week 2 Portfolio</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Week4.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week 4 Policy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>