<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CASA0023 Learning Diary - 7&nbsp; Week 7 Classification II</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Week8.html" rel="next">
<link href="./Week6.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">CASA0023 Learning Diary</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week 7 Classification II</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Introduction</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Week 1 Getting started with remote sensing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Week 2 Portfolio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week 3 Corrections</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week 4 Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week 5 Google Earth Engine I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week6 Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week7.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week 7 Classification II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Week 8 Temperature and policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary"><span class="toc-section-number">7.1</span>  Summary</a>
  <ul class="collapse">
  <li><a href="#new-classification-methods" id="toc-new-classification-methods" class="nav-link" data-scroll-target="#new-classification-methods"><span class="toc-section-number">7.1.1</span>  New classification methods</a></li>
  <li><a href="#accuracy-assessment" id="toc-accuracy-assessment" class="nav-link" data-scroll-target="#accuracy-assessment"><span class="toc-section-number">7.1.2</span>  Accuracy assessment</a></li>
  <li><a href="#spatial-cross-validation" id="toc-spatial-cross-validation" class="nav-link" data-scroll-target="#spatial-cross-validation"><span class="toc-section-number">7.1.3</span>  Spatial cross validation</a></li>
  <li><a href="#summary-of-practical" id="toc-summary-of-practical" class="nav-link" data-scroll-target="#summary-of-practical"><span class="toc-section-number">7.1.4</span>  Summary of practical</a></li>
  </ul></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="toc-section-number">7.2</span>  Application</a></li>
  <li><a href="#reflection" id="toc-reflection" class="nav-link" data-scroll-target="#reflection"><span class="toc-section-number">7.3</span>  Reflection</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week 7 Classification II</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this week, the content looks like a continuation of last week’s content about remote sensing image classification. Looking back to last week, our group was going to learn pixel based classification, that is, pixels are divided into different categories, and this week we learned two new classification methods. In addition, we have studied several methods for evaluating the accuracy of classification results. In order to improve the accuracy and precision of classification, we also learned spatial cross validation. To better understand the relevant knowledge, I also looked up the relevant applications.</p>
<section id="summary" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.1</span> Summary</h2>
<p>The summary part of learning diary in this week has been divided by four parts, including new classification methods (object-based image analysis and sub pixel analysis), accuracy assessment, cross validation, and summary of practical.</p>
<section id="new-classification-methods" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="new-classification-methods"><span class="header-section-number">7.1.1</span> New classification methods</h3>
<ul>
<li><p><strong>Object-based image analysis (OBIA)</strong></p>
<p>OBIA is commonly to see when doing remote sensing analysis, especially classification. Simple Linear Iterative Clustering (SLIC) will be doing first usually to generate “blocks”, which by giving some regular points in the acquired image, and according to the distance or similarity between each pixel in the image and these points, or other characteristics, small object is generated based on each regular point as centroid, and then remove these centroids. Comparing to single pixel, each segmentation (or object) we get after object-based image analysis has more information about the spectral information and spatial information <span class="citation" data-cites="blaschke2010">(<a href="references.html#ref-blaschke2010" role="doc-biblioref">Blaschke 2010</a>)</span>. Figure 1 shows the objects created when doing OBIA classification.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/week7_summary_OBIA_1.png" class="img-fluid figure-img" width="400"></p>
<p></p><figcaption class="figure-caption">Fig.1 Objects created for doing object-based classification. Source: <span class="citation" data-cites="gisgeography2022">GISGeography (<a href="references.html#ref-gisgeography2022" role="doc-biblioref">2022</a>)</span></figcaption><p></p>
</figure>
</div>
<ul>
<li><p><strong>Sub pixel analysis</strong></p>
<p>Sometimes the resolution of remote sensing image we acquire is not high enough, or the object we want to explore is too small to take up most part of the pixel, so a pixel may contain more than one type of object. In this case, sub pixel analysis (similar as Spectral Mixture Analysis or Linear spectral unmixing) can be tried to classify images more accurately. Mainly used to determine the proportion or abundance of landcover per pixel. According to the following formula, we can calculate the fractions of different types of objects in this pixel, which can help us make decision on which category should this pixel belongs to or how to divide this pixel.</p>
<p><img src="week7_summary_sub_1.png" class="img-fluid" width="400"></p></li>
</ul>
</section>
<section id="accuracy-assessment" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="accuracy-assessment"><span class="header-section-number">7.1.2</span> Accuracy assessment</h3>
<p>After classification, we need to assess the accuracy of the classification results, and usually in remote sensing, we use confusion matrix and we consider that the true positive and true negative part of the classification results show where the model is correct, and false positive and false negative parts show where the model is incorrect.</p>
<ul>
<li><strong>Producer accuracy (PA), user’s accuracy (UA), and overall accuracy (OA)</strong></li>
</ul>
<p>We can calculate producer accuracy (PA), user’s accuracy (UA), and overall accuracy (OA) based on the values in the confusion matrix and the formulas below. Producer accuracy represents how well the producer of map classified the image, while user’s accuracy refers how well the users can use the classified map.</p>
<p><span class="math display">\[
PA=\frac{True positive}{True positive + False negative}
\]</span></p>
<p><span class="math display">\[
UA=\frac{True positive}{True positive + False positive}-False positive
\]</span></p>
<p><span class="math display">\[
OA=\frac{True positive+True negative}{True positive +True negative+ False positive + False negative}
\]</span></p>
<ul>
<li><strong>Kappa coefficient</strong></li>
</ul>
<p>Another commonly used accuracy assessment is Kappa coefficient, which can express the accuracy of image classification compared to chance results. The range of Kappa is from 0 to 1. However, there are two issues of Kappa coefficient, one is no unified standard for evaluating the value of Kappa so far, the other is different overall accuracy may correspond to different value ranges of Kappa, which may lead to different results of the same analysis conducted by different people.</p>
<ul>
<li><strong>F1 score</strong></li>
</ul>
<p>Since it is hard to get perfect result with both high UA and PA, combine them into one measurement, that is, F1 score. The range of F1 score is from 0 to 1, and 1 means the perfect performance. According to the following formula, it is clear that there is no True negative in it. In addition, it might not be suitable for the data is unbalanced with much more negatives results.</p>
<p><span class="math display">\[
F1 =\frac{Truepositive}{True positive + \frac{False positive+False negative}{2}}
\]</span></p>
<ul>
<li><strong>Receiver Operating Characteristic Curve</strong></li>
</ul>
<p>Receiver operating characteristic curve (ROC curve) generated based on true positive rate and false positive rate according to the following formulas. Then, we need to calculate the area user the ROC curve, which is the accuracy. This method requires a combination of train and test split or cross validation.</p>
<p><span class="math display">\[
Truepositiverate=\frac{True positive}{True positive + False negative}
\]</span></p>
<p><span class="math display">\[
Falsepositive rate=\frac{False positive}{False positive + True negative}
\]</span></p>
</section>
<section id="spatial-cross-validation" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="spatial-cross-validation"><span class="header-section-number">7.1.3</span> Spatial cross validation</h3>
<p>When doing cross validation based on spatial data, most of time, we need to consider the spatial dependence issue. There should not be spatial dependence between training and test sets in each iteration. According to the relevant content that we learnt in CASA0005 last term, we can use Moran’s I to detect whether there is spatial autocorrelation between training set and testing set.</p>
<p>What is different from cross validation is that we do spatially partition the folded data from cross validation and do clustering analysis to get the final training and testing sets in each fold to avoid the data points in training set and testing set are too close, which can also avoid to get the overfitting classification result.</p>
<ul>
<li><strong>Additional content:</strong> In the lecture, there is an application of spatial cross validation combined with Support Vector Machine (SVM) to classify the image. C and gamma are two important parameters in this method, which C is for changing the slope of classification line between two classes, and gamma is for controlling the influence of a training point in that image. We can get C and gamma after each run in each fold based on different split, and then use the C and gamma to classify the image.</li>
</ul>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notice
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Iteration for doing OBIA is needed, and the regular points move 4 to 10 times is better in each iteration.</li>
<li>It is hard to evaluate the accuracy of the fraction results of sub pixel analysis, and the results might not always good, which need to be considered when choosing this method. In the practical part, there are two solutions introduced for this issue.</li>
<li>No perfect answer for how to classify the image, sometimes we can make decision based on the classification is more important to the producer or the user, thus reconciling PA and OA.</li>
<li>Errors of omission has similar function as producer’s accuracy, and errors of commission has similar function as user’s accuracy.</li>
</ol>
</div>
</div>
</section>
<section id="summary-of-practical" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="summary-of-practical"><span class="header-section-number">7.1.4</span> Summary of practical</h3>
<p>In this week, the practical can be divided into two main parts, including data pre-processing and two new classification methods, one is based on sub pixel analysis, the other is based on object-based image analysis. Main workflow of practical in this week shows in the following Figure 2.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week7_prac_workflow.png" class="img-fluid figure-img" width="600"></p>
<p></p><figcaption class="figure-caption">Fig.2 Workflow of practical in week 7</figcaption><p></p>
</figure>
</div>
<ul>
<li><strong>Workflow</strong></li>
</ul>
<p>In the first part of the practical, we did similar data pre-processing as the methods we used in the previous weeks. One interesting thing during the process of remove the pixels covered by clouds or with cloud shadow by using a cloud mask function we defined. However, from the result (Figure 3), the deleted pixels are not affected by the cloud, it may be that the sand or buildings are misjudged to be clouds due to high reflectance. Since visual interpretation found that there was no significant cloud cover within the study area, this step of cloud removal was omitted from data pre-processing the image in order to preserve the complete image.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week7_prac_cloudremove_1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.3 Image after cloud masking</figcaption><p></p>
</figure>
</div>
<ul>
<li><strong>Sub pixel analysis</strong></li>
</ul>
<p>In the sub pixel analysis section, the process mentioned in the class is reviewed, including first creating different types of land-use points or manually setting some numerical values, then extracting and averaging different categories of points, and calculating the fraction corresponding to each land-use type in each pixel according to the formula mentioned above. After obtaining the proportion of different land use types in each pixel, the category that accounts for more than 50% of each pixel is selected as the land use type of this pixel. The code of reclassifying shows in the following code chunk. Then we get the classification result of Figure 4 below, where the gray area represents urban area, blue area represents water, and the green parts represent the vegetation.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode javascript code-with-copy"><code class="sourceCode javascript"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">// reclassify</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">var</span> reclassified_urban <span class="op">=</span> fractions_constrained<span class="op">.</span><span class="fu">expression</span>(<span class="st">'b(0) &gt;.5 ? 1 : 0'</span>)<span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">var</span> reclassified_vegetation <span class="op">=</span> fractions_constrained<span class="op">.</span><span class="fu">expression</span>(<span class="st">'b(1) &gt;.5 ? 2 : 0'</span>)<span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">var</span> reclassified_water <span class="op">=</span> fractions_constrained<span class="op">.</span><span class="fu">expression</span>(<span class="st">'b(3) &gt;.5 ? 3 : 0'</span>)<span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">// add all images together and cliped by the boundary of study area</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">var</span> reclassified_all <span class="op">=</span> reclassified_urban<span class="op">.</span><span class="fu">add</span>(reclassified_urban)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                                <span class="op">.</span><span class="fu">add</span>(reclassified_bare_vegetation)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                                <span class="op">.</span><span class="fu">add</span>(reclassified_water)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                                <span class="op">.</span><span class="fu">clip</span>(Daressalaam)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As can be seen from Figure 4, this classification result is not accurate. Many central areas in the study area were classified into water, but after the comparison with the remote sensing image I selected, this classification result is obviously wrong, that’s might because of the feature points I selected are not accurate enough. Besides that, most of the peripheral study area was classified into vegetation, and the density and texture of the vegetation distribution should not be uniform, there should be some gathered forests and vegetation scattered between buildings. Therefore, this part of the classification is inaccurate, which may be due to the lower resolution of the image, and higher resolution data should be tried.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week7_prac_subpixelclassify.png" class="img-fluid figure-img" width="400"></p>
<p></p><figcaption class="figure-caption">Fig.4 Classification result based on sub pixel analysis</figcaption><p></p>
</figure>
</div>
<ul>
<li><strong>Object-based image analysis</strong></li>
</ul>
<p>The main purpose of this part is to try classify the image based on object-based image analysis, not in pixel scale, so first generate objects from pixels in the image, and then use appropriate classification methods to classify objects, for instance, K-means classification. Therefore, I first generated a spectral gradient based on the difference in spectral values, then clustered those objects based on Simple non-iterative clustering (SNIC), and check the rationality of clustering results based on the distribution of NDVI of each cluster. In this practical case, I used the CART method for image classification, which is shown in Figure 5 below. The gray area is the urban area, the green area is vegetation, and the blue area is water.</p>
<p>It can be seen from Figure 5 that the urban area is mainly distributed in the center of the study area, and there are also building agglomerations where it is off-center area, while the vegetation is mainly distributed in the off-center area of the study area. However, compared with the results of visual interpretation of images, in addition to the basic accuracy of the identification of water bodies, the accuracy of classification of other land use types is still relatively low. I think it may be due to the large size of the objects I set, and the mixed distribution of local land use types, so the classification results are not very accurate. In addition, it may be that the land use type I set is inadequate, and I should take into account the local land use characteristics, add consideration to bare land, and refine the classification of vegetation, which is divided into grassland and forest. At the same time, perhaps due to the low robustness of CART, switching to random forest or other ensemble learning may help improve classification accuracy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="week7_prac_finalOBIAclassify_part.png" class="img-fluid figure-img" width="400"></p>
<p></p><figcaption class="figure-caption">Fig.5 Classification result based on object-based image classification and CART</figcaption><p></p>
</figure>
</div>
<p>For Figure 4 and Figure 5 above, the difference between the two classification results can be clearly contrasted, because the smallest unit scales of the two classification methods are different, so I think the biggest advantage of sub pixel analysis classification is that it can identify mixed land use types, for example, the urban areas (buildings) distributed in large areas of vegetation in Figure 4. The various types of land use in Figure 5 are composed of small irregular clusters. For different data and research questions, the applicable classification methods would also be different, so many factors, the characteristics and shortcomings of different classification methods should be considered when doing classification.</p>
<p>However, the above evaluation and comparison of classification results based on sub pixel analysis and object-based image analysis are compared with the visual interpretation results of images, and cannot be used as quantitative, scientific and accurate evaluation results. Therefore, this is also a deficiency of this week’s practical section, which does not apply any of the methods learned in the class this week to assess the accuracy of the classification results.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notice
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>QA band is for removing cloud pixels and avoid misclassify.</li>
</ol>
</div>
</div>
<ul>
<li><strong>Questions</strong></li>
</ul>
<p>So far no question related to the content in this week.</p>
</section>
</section>
<section id="application" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="application"><span class="header-section-number">7.2</span> Application</h2>
<p>In this week’s Application section, I added two papers on sub-pixel-level classification and object-based image analysis classification, respectively. In addition to summarizing the content of each literature, the data and methods used in the study and other aspects are discussed as well.</p>
<ol type="1">
<li><strong>Application of sub pixel analysis classification</strong></li>
</ol>
<ul>
<li>Summary</li>
</ul>
<p><span class="citation" data-cites="deng2020">Deng and Zhu (<a href="references.html#ref-deng2020" role="doc-biblioref">2020</a>)</span> conducted an urban transition study on a county in southern New York state. According to the Landsat data they obtained from 2000 to 2014, the method combined with Continuous Sub-pixel Monitoring (CSM) improves the accuracy of urban impervious layer mapping based on sub pixel level, and random forest classification continuously monitors urban transitions in time and space <span class="citation" data-cites="deng2020">(<a href="references.html#ref-deng2020" role="doc-biblioref">Deng and Zhu 2020</a>)</span>. Figure 5 below shows how sensitive this method is for monitoring urban transitions. For example, the green circles marked in Figures 5 A and B were a complete vacant lot in 2006, while Figures 5 C and D show that in 2014 the the vacant lot in the same location was divided into three parts and developed to varying degrees. This change is not only very clear on the NAIP orthophotos, but the difference can also be clearly seen by comparing A and C. According to the results obtained in this study, this method can achieve continuous monitoring of urban transition in areas undergoing urbanization with relatively high accuracy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/week7_application_sub_1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.6 A county in New York State as the percentage increase in the percentage of impervious surfaces from 2006 to 2014. A: 2006CSM； B: 2006NAIP orthophoto; C:2014 CSM; D: 2014NAIP orthophoto.</figcaption><p></p>
</figure>
</div>
<ul>
<li>Discussion</li>
</ul>
<p>According to this research, I think the data they chose is appropriate. First of all, because the Landsat series of satellites has been used for a long time, and relatively early data can be obtained. Secondly, it is easier to obtain long-term, continuous data as open source satellite data <span class="citation" data-cites="deng2020">(<a href="references.html#ref-deng2020" role="doc-biblioref">Deng and Zhu 2020</a>)</span>. In addition, I deem that it is reasonable for the authors to choose to analyze images based on sub pixel levels. That is because the resolution of the Landsat series images used in the study is about 30 to 60 meters <span class="citation" data-cites="usgs">(<a href="references.html#ref-usgs" role="doc-biblioref">USGS, n.d.</a>)</span>, it will cause serious mixing pixel problems in urbanization areas with high population and facility density <span class="citation" data-cites="deng2020">(<a href="references.html#ref-deng2020" role="doc-biblioref">Deng and Zhu 2020</a>)</span>. Therefore, sub pixel level is acceptable if they need to accurately monitor urban transitions. In addition, the Landsat imagery was classified based on the ensemble learning method of random forest, which makes the classification results relatively robust. However, due to the needs for long-term, frequent and accurate monitoring, the demand for data is huge, and with the extension of monitoring time, there may be big problems in data storage.</p>
<ol start="2" type="1">
<li><strong>Application of OBIA classification</strong></li>
</ol>
<ul>
<li>Summary</li>
</ul>
<p><span class="citation" data-cites="phiri2018">Phiri et al. (<a href="references.html#ref-phiri2018" role="doc-biblioref">2018</a>)</span> classified the pansharpened images of Landsat 8 and standard OLI 8 images after atmospheric and topological corrections based on Object-based image analysis and random forest classification to evaluate the impact of different data pre-processing methods on land cover classification accuracy by comparing different Z statistics. <span class="citation" data-cites="phiri2018">Phiri et al. (<a href="references.html#ref-phiri2018" role="doc-biblioref">2018</a>)</span> chose the random forest-based classification method because after trying 5 different machine learning classification methods, including CART, they found that the results of the random forest-based classification method were the most accurate. It is clear from the Figure 6 E that it is more accurate for the classification of secondary forests in the northeast direction in the image, and the classification of dry agriculture in the middle of the image is significantly better. Therefore, they concluded that the accuracy of image classification is not only affected by the resolution of the original image, but also has an obvious relationship with the method of data pre-processing and classification of images in this case.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/week7_application_OBIA_1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">Fig.7 Land cover classification result of Landsat 8 pandsharpened images based on different data pre-processing methods (A: no pre-processing; E: atmospheric and topographic correction).</figcaption><p></p>
</figure>
</div>
<ul>
<li>Discussion</li>
</ul>
<p>This study can be contrasted with the third part of practical above. When doing practical, I thought one of the reasons why classification is not very accurate is that the CART classification without high robust, and could be replaced by random forest classification. I think the reason why the random forest classification is more appropriate is that this method uses the least variables, and its final result is based on multiple decision trees, which can avoid the occurrence of overfitting, and the robustness of the results and the accuracy of classification are relatively high <span class="citation" data-cites="phiri2018">(<a href="references.html#ref-phiri2018" role="doc-biblioref">Phiri et al. 2018</a>)</span>. The authors chose the OBIA classification method might because it can combine many aspects of information, such as texture, size, shape, and other aspects <span class="citation" data-cites="phiri2017">(<a href="references.html#ref-phiri2017" role="doc-biblioref">Phiri and Morgenroth 2017</a>)</span>, so it is more suitable for studying the complex land cover type study area. However, OBIA is more suitable for high-resolution remote sensing images, although there are also many studies that chose to classify image with relatively low resolution like Landsat images based on OBIA classification <span class="citation" data-cites="phiri2017">(<a href="references.html#ref-phiri2017" role="doc-biblioref">Phiri and Morgenroth 2017</a>)</span>. It is not necessarily the most appropriate method, because it may be difficult to identify information such as texture when the resolution is not high, and the advantages of OBIA cannot be fully played, which is not necessary to carry out relative to pixel-level classificationMore complex object-based classification.</p>
</section>
<section id="reflection" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="reflection"><span class="header-section-number">7.3</span> Reflection</h2>
<p>What is interesting?</p>
<p>I think what’s more interesting about this week are the two new classifications: sub pixel classification and OBIA classification. They are not based on traditional pixel classification methods, but break the boundaries of pixels to segment remote sensing images. Another interesting thing is, in the practical part, it is quite obviously that the classification results of sub pixel analysis and the classification based on OBIA have several differences. However, I haven’t figured out why the area in the middle of the image is classified as water when based on sub pixel classification, because I did not select feature points in the water category nearby, and the color of the features corresponding to the water feature points selected is significantly different. Therefore, to figure out this question, I may learn more about the fundamentals and applications of the sub pixel classification after class in future.</p>
<p>What is useful in the future?</p>
<p>For the classification methods that have been studied a lot by previous scholars, there should not be a completely useless situation for those classification methods and different classification levels, but it may not be suitable for all cases, resulting in poor classification accuracy. For example, based on what has been learned and learned this week, OBIA may be better suited to classify higher-resolution imagery, which can better exploit its advantages, such as extracting informal buildings and formal buildings, distinguishing forests from grasslands, and so on. In addition, random forest and other ensemble learning methods may be the classification methods I often use when performing image classification, because they can help me improve the accuracy of classification to a certain extent and reduce the occurrence of overfitting.</p>
<p>What can be instead?</p>
<p>I think that the easier alternative in the content of this week is the methods of assessing the accuracy of classification results, because there are many evaluation methods, as long as we choose one of them to calculate the classification accuracy of different categories and facilitate comparison. For example, <span class="citation" data-cites="phiri2018">Phiri et al. (<a href="references.html#ref-phiri2018" role="doc-biblioref">2018</a>)</span> uses Z statistics to characterize the accuracy of the classification, but they can also obtain producer’s accuracy, user’s accuracy, and overall accuracy through the confusion matrix to assess the method, compare differences, and analyze the results.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-blaschke2010" class="csl-entry" role="doc-biblioentry">
Blaschke, T. 2010. <span>“Object Based Image Analysis for Remote Sensing.”</span> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 65 (1): 2–16. <a href="https://doi.org/10.1016/j.isprsjprs.2009.06.004">https://doi.org/10.1016/j.isprsjprs.2009.06.004</a>.
</div>
<div id="ref-deng2020" class="csl-entry" role="doc-biblioentry">
Deng, Chengbin, and Zhe Zhu. 2020. <span>“Continuous Subpixel Monitoring of Urban Impervious Surface Using Landsat Time Series.”</span> <em>Remote Sensing of Environment</em> 238 (March): 110929. <a href="https://doi.org/10.1016/j.rse.2018.10.011">https://doi.org/10.1016/j.rse.2018.10.011</a>.
</div>
<div id="ref-gisgeography2022" class="csl-entry" role="doc-biblioentry">
GISGeography. 2022. <span>“OBIA - Object-Based Image Analysis (GEOBIA).”</span> <a href="https://gisgeography.com/obia-object-based-image-analysis-geobia/">https://gisgeography.com/obia-object-based-image-analysis-geobia/</a>.
</div>
<div id="ref-phiri2017" class="csl-entry" role="doc-biblioentry">
Phiri, Darius, and Justin Morgenroth. 2017. <span>“Developments in Landsat Land Cover Classification Methods: A Review.”</span> <em>Remote Sensing</em> 9 (9): 967. <a href="https://doi.org/10.3390/rs9090967">https://doi.org/10.3390/rs9090967</a>.
</div>
<div id="ref-phiri2018" class="csl-entry" role="doc-biblioentry">
Phiri, Darius, Justin Morgenroth, Cong Xu, and Txomin Hermosilla. 2018. <span>“Effects of Pre-Processing Methods on Landsat OLI-8 Land Cover Classification Using OBIA and Random Forests Classifier.”</span> <em>International Journal of Applied Earth Observation and Geoinformation</em> 73 (December): 170–78. <a href="https://doi.org/10.1016/j.jag.2018.06.014">https://doi.org/10.1016/j.jag.2018.06.014</a>.
</div>
<div id="ref-usgs" class="csl-entry" role="doc-biblioentry">
USGS. n.d. <span>“What Are the Band Designations for the Landsat Satellites? | u.s. Geological Survey.”</span> <a href="https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites">https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Week6.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week6 Classification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Week8.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Week 8 Temperature and policy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>